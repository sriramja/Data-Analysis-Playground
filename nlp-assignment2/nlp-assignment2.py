# -*- coding: utf-8 -*-
"""220 - Assignment 2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LdH6KFzuMcV2Uuqhz_aUuaUN9zPD2C-7

Part - 1
"""

#!wget https://github.com/PoorvaRane/Emotion-Detector/blob/master/ISEAR.csv

# Load the ISEAR corpus using Python.

import pandas as pd

df = pd.read_csv("/content/ISEAR.csv",header=None)
df = df[[0,1]]
df.columns = ['Emotion','Line']

# Normalizing data
df.at[7499,'Emotion'] = 'guilt'

# Creating Dataframe
df['Length'] = 0
for j in range(0,len(df['Emotion'])):
  df['Length'][j] = len(df['Line'][j].split())

# Removing Stopwords, special characters and making words lowercase.

import nltk
from nltk.corpus import stopwords

nltk.download('stopwords')

stop_words = set(stopwords.words('english'))
stop_words.add('I')
stop_words.add('mr')
stop_words.add('When')

for i in range(0,len(df['Line'])):

  tokensWithoutStopwords = [word for word in df['Line'][i].split() if word not in stop_words]
  tokensWithoutStopwordsAlNum = [e for e in tokensWithoutStopwords if e.isalnum()]
  finaltokens = [e.lower() for e in tokensWithoutStopwordsAlNum]
  df['Line'][i] = " ".join(finaltokens)
print(df['Line'])

pd.set_option('display.max_colwidth', None)

# list of emotions in dataframe
emotions = list(set(df['Emotion']))
emotions
emotions.sort()

# Getting mean lengths
for j in range(0,len(df['Emotion'])):
  avg_df = df.groupby('Emotion').mean()

avg_df

print(list(avg_df['Length']))

# Getting max and min lengths
grouped_df = df.groupby('Emotion')
maximums = grouped_df.max()
minimums = grouped_df.min()
print(maximums)
print(minimums)

#Create new dataframe before outputting as csv

import csv
import pandas
import numpy as np

pd.set_option('display.max_colwidth', None)
temp_df = pd.DataFrame(np.array([emotions, list(maximums['Length']), list(minimums['Length']),list(avg_df['Length'])]))
temp_df.reset_index(drop=True, inplace=True)

print(temp_df.transpose())

#Add it to CSV file using \t as delimiter
temp_df.transpose().to_csv('output.csv', sep= '\t', header=['Emotions', 'Max Length', 'Min Length','Average Length'], index=False)

for i in range(0,len(emotions)):
  abc = df[df["Emotion"] == emotions[i]]
  print(abc[['Emotion','Length']].max())
  print(abc[['Emotion',"Line",'Length']].min())

# For each type of emotion, analyze the distribution (frequency) of tokens

from itertools import chain
from collections import Counter
#flatten_list = list(chain.from_iterable(ini_list))

anger = []
disgust = []
fear = []
guilt = []
joy = []
sadness = []
shame = []

for i in range(0,len(df['Emotion'])):
  if df['Emotion'][i] == 'anger':
    anger.append(df['Line'][i].split())    
  elif df['Emotion'][i] == 'disgust':
    disgust.append(df['Line'][i].split())
  elif df['Emotion'][i] == 'fear':
    fear.append(df['Line'][i].split()) 
  elif df['Emotion'][i] == 'guilt':
    guilt.append(df['Line'][i].split())
  elif df['Emotion'][i] == 'joy':
    joy.append(df['Line'][i].split())
  elif df['Emotion'][i] == 'sadness':
    sadness.append(df['Line'][i].split())     
  elif df['Emotion'][i] == 'shame':
    shame.append(df['Line'][i].split())

anger_words = list(chain.from_iterable(anger))
disgust_words = list(chain.from_iterable(disgust))
fear_words = list(chain.from_iterable(fear))
guilt_words = list(chain.from_iterable(guilt))
joy_words = list(chain.from_iterable(joy))
sadness_words = list(chain.from_iterable(sadness))
shame_words = list(chain.from_iterable(shame))

print("Anger - Top-50 Word Frequencies:", dict(Counter(anger_words).most_common(50)))
print("Disgust - Top-50 Word Frequencies:", dict(Counter(disgust_words).most_common(50)))
print("Fear - Top-50 Word Frequencies:", dict(Counter(fear_words).most_common(50)))
print("Guilt - Top-50 Word Frequencies:", dict(Counter(guilt_words).most_common(50)))
print("Joy - Top-50 Word Frequencies:", dict(Counter(joy_words).most_common(50)))
print("Sadness - Top-50 Word Frequencies:", dict(Counter(sadness_words).most_common(50)))
print("Shame - Top-50 Word Frequencies:", dict(Counter(shame_words).most_common(50)))

print("\n-----------------------------------------------------------------------\n")

# For each type of emotion, calculate the vocabulary size
anger_vocab = set(anger_words)
disgust_vocab = set(disgust_words)
fear_vocab = set(fear_words)
guilt_vocab = set(guilt_words)
joy_vocab = set(joy_words)
sadness_vocab = set(sadness_words)
shame_vocab = set(shame_words)

print("Anger - Vocabulary:", len(anger_vocab))
print("Disgust - Vocabulary:", disgust_vocab)
print("Fear - Vocabulary:", fear_vocab)
print("Guilt - Vocabulary:", guilt_vocab)
print("Joy - Vocabulary:", joy_vocab)
print("Sadness - Vocabulary:", sadness_vocab)
print("Shame - Vocabulary:", shame_vocab)

"""**Part**- 2"""

# PART - 2

# Import Sem-eval dataset (https://www.kaggle.com/azzouza2018/semevaldatadets) using Python.

# Use data file to use for assignment:  semeval-2017-train.csv
data = pd.read_csv("/content/semeval-2017-train.csv", delimiter='\t')

data.head()

# Print the sentences which contains URLs and hashtags
import re

for i in range(0,len(data['text'])):
  if re.match('(http.?:\/\/)?(www\.)?([a-zA-Z0-9]+(-?[a-zA-Z0-9])*\.)+[\w]{2,}(\/\S*)?', data['text'][i], re.MULTILINE):
    print("Contains URL:", data['text'][i], '\n')
  elif re.match('#[a-zA-Z0-9]+(?:\s|$)', data['text'][i], re.MULTILINE):
    print("Contains Hashtag:", data['text'][i], '\n')

# Print all neutral sentences (annotated with 0)

print('\nNeutral sentences:')

for i in range(0,len(data['label'])):
  if data['label'][i] == 0:
    print(data['text'][i],'\n')

# Find the distribution of positive, negative and neutral sentences

from collections import Counter

sentiment = []

for i in range(1,len(data['label'])):
  sentiment.append(data['label'][i])

sentiment_count = dict(Counter(sentiment))
print("Frequency distribution of sentiments:", sentiment_count)
print("Frequency of Positive:", sentiment_count[1])
print("Frequency of Neutral:",  sentiment_count[0])
print("Frequency of Negative:",  sentiment_count[-1])

# PART - 3

#  Load the XML file 'movies-new.xml' using Python DOM parsing and do the following:
# Find all the movie title, year, rating, and description and print the output.

from xml.dom.minidom import parse
import xml.dom.minidom

DOMTree = xml.dom.minidom.parse("/content/movies-new.xml")
collection = DOMTree.documentElement
if collection.hasAttribute("shelf"):
   print("Root element : %s" % collection.getAttribute("shelf"))

movies = collection.getElementsByTagName("movie")
movies_json = []

for movie in movies:
   print("Movie")
   if movie.hasAttribute("title"):
      print ("Title: %s" % movie.getAttribute("title"))
   year = movie.getElementsByTagName('year')[0]
   print ("Year: %s" % year.childNodes[0].data)
   rating = movie.getElementsByTagName('rating')[0]
   print ("Rating: %s" % rating.childNodes[0].data)
   description = movie.getElementsByTagName('description')[0]
   print ("Description: %s" % description.childNodes[0].data,"\n--------------------\n")
   movies_json.append({'title': movie.getAttribute("title"), 'year':year.childNodes[0].data, 'rating': rating.childNodes[0].data, 'description': description.childNodes[0].data})

# Output them in a JSON file 'movies.json' as follows:
# {title: "a title", year:1982, rating: PG, description: "a description}

import json

with open ("movies.json", mode = "w") as j:
  j.write(json.dumps(movies_json))

"""Part - 4"""

# PART -4
# Now load the XML file 'movies-new.xml' using Element-Tree and do the following:
# Find the movie title, and year of all Action movies and print them 

import xml.etree.ElementTree as ET

tree = ET.parse('movies-new.xml')
root = tree.getroot()
movies = []
year = []

for child in root:
    if child.attrib['category'] == 'Action':
      for tag in child.iter('movie'):
        movies.append(tag.attrib['title'])
      for tag in child.iter('year'):
        year.append(tag.text)

print("Action Movies:\n")
for i in range(0,len(movies)):
  print("Movie:", movies[i])
  print("Year:", year[i])